{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd                                      # for data loading and manipulation :contentReference[oaicite:2]{index=2}\n",
    "import torch                                             # for tensor/device management\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM  # for model inference :contentReference[oaicite:3]{index=3}\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # for evaluation metrics :contentReference[oaicite:4]{index=4}\n",
    "from sklearn.metrics import top_k_accuracy_score        # for Top-K accuracy :contentReference[oaicite:5]{index=5}\n",
    "from sklearn.metrics import ConfusionMatrixDisplay      # for plotting confusion matrix :contentReference[oaicite:6]{index=6}\n",
    "from tqdm.notebook import tqdm                           # for progress bar display\n",
    "import matplotlib.pyplot as plt                          # for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "base_model_id        = \"meta-llama/Llama-2-3b-hf\"           # Hugging Face ID of base model\n",
    "finetuned_model_dir  = \"outputs\"                           # Path to locally fine-tuned model\n",
    "validation_csv       = \"data/processed/symptom-disease-validation-dataset.csv\"  # Validation data path\n",
    "max_gen_length       = 128                                 # Max tokens for generation\n",
    "num_return_sequences = 1                                   # Return only the top sequence\n",
    "device               = 0 if torch.cuda.is_available() else -1  # Use GPU if available :contentReference[oaicite:7]{index=7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f31d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load data\n",
    "val_df = pd.read_csv(validation_csv)                        # Read CSV into DataFrame :contentReference[oaicite:8]{index=8}\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea332f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model + Pipeline setup\n",
    "# Base model pipeline\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "base_model     = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "base_pipe      = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=base_tokenizer,\n",
    "    device=device,\n",
    "    max_length=max_gen_length,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    pad_token_id=base_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Fine-tuned model pipeline\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
    "ft_model     = AutoModelForCausalLM.from_pretrained(finetuned_model_dir, device_map=\"auto\")\n",
    "ft_pipe      = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=ft_tokenizer,\n",
    "    device=device,\n",
    "    max_length=max_gen_length,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    pad_token_id=ft_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Inference loop\n",
    "results = []\n",
    "for idx, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Running inference\"):  # tqdm progress bar :contentReference[oaicite:9]{index=9}\n",
    "    symptoms   = row[\"text\"]\n",
    "    true_label = row[\"label\"]\n",
    "\n",
    "    # Format prompt as chat instruction\n",
    "    prompt = f\"[s][INST] Symptoms: {symptoms} [/INST]\"\n",
    "\n",
    "    # Base model prediction\n",
    "    base_out = base_pipe(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "    base_pred = base_out.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    # Fine-tuned model prediction\n",
    "    ft_out = ft_pipe(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "    ft_pred = ft_out.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    results.append({\n",
    "        \"symptoms\": symptoms,\n",
    "        \"true\":     true_label,\n",
    "        \"base_pred\": base_pred,\n",
    "        \"ft_pred\":   ft_pred\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4236a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Quantitative evaluation\n",
    "# Exact-match accuracy\n",
    "base_acc = accuracy_score(comp_df[\"true\"], comp_df[\"base_pred\"])\n",
    "ft_acc   = accuracy_score(comp_df[\"true\"], comp_df[\"ft_pred\"])\n",
    "print(f\"Base Model Accuracy:      {base_acc:.2%}\")           # accuracy_score :contentReference[oaicite:10]{index=10}\n",
    "print(f\"Fine-tuned Model Accuracy:{ft_acc:.2%}\")\n",
    "\n",
    "# Top-1 accuracy (indices coding)\n",
    "labels = list(comp_df[\"true\"].unique())\n",
    "label2idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "y_true = [label2idx[t] for t in comp_df[\"true\"]]\n",
    "y_base = [label2idx.get(p, -1) for p in comp_df[\"base_pred\"]]\n",
    "y_ft   = [label2idx.get(p, -1) for p in comp_df[\"ft_pred\"]]\n",
    "print(\"Base Top-1 Accuracy:\", top_k_accuracy_score(y_true, [y_base], k=1))  # top_k_accuracy_score :contentReference[oaicite:11]{index=11}\n",
    "print(\"Fine-tuned Top-1 Accuracy:\", top_k_accuracy_score(y_true, [y_ft], k=1))\n",
    "\n",
    "# Classification report for fine-tuned model\n",
    "print(\"\\nFine-tuned Model Classification Report:\")\n",
    "print(classification_report(y_true, y_ft, target_names=labels))            # classification_report :contentReference[oaicite:12]{index=12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Confusion matrix plot\n",
    "cm = confusion_matrix(y_true, y_ft)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(cmap=\"Blues\", ax=ax)\n",
    "plt.title(\"Fine-tuned Model Confusion Matrix\")\n",
    "plt.show()                                                                 # visual using ConfusionMatrixDisplay :contentReference[oaicite:13]{index=13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8cfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save comparison dataframe\n",
    "comp_df.to_csv(\"outputs/model_comparison.csv\", index=False)                # DataFrame.to_csv :contentReference[oaicite:14]{index=14}\n",
    "print(\"Saved comparison results to outputs/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad585504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7e521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5844d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

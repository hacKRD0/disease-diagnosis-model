# configs/model/smolLM_360M.yaml
name: "smolLM_360M"
hf_checkpoint: "HuggingFaceTB/SmolLM-360M"   # Hugging Face model ID :contentReference[oaicite:0]{index=0}
tokenizer_checkpoint: "HuggingFaceTB/SmolLM-360M"

# PEFT / LoRA settings
peft:
  method: "lora"
  r: 2                                   # LoRA rank :contentReference[oaicite:1]{index=1}
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ["q_proj","v_proj"]    # modules to apply LoRA :contentReference[oaicite:2]{index=2}
  modules_to_save: ["lm_head"]

# Model kwargs (passed to from_pretrained)
model_kwargs:
  torch_dtype: "auto"
  device_map: "auto"
